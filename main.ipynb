{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load và xử lý dữ liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dữ liệu\n",
    "train0_data = pd.read_json('data/train_0.json', lines=True)\n",
    "train1_data = pd.read_json('data/train_1.json', lines=True)\n",
    "train2_data = pd.read_json('data/train_2.json', lines=True)\n",
    "train3_data = pd.read_json('data/train_3.json', lines=True)\n",
    "train4_data = pd.read_json('data/train_4.json', lines=True)\n",
    "total_train_data = pd.concat([train0_data, train1_data, train2_data, train3_data, train4_data])\n",
    "\n",
    "#Xử lý dữ liệu thiếu \n",
    "total_train_data = total_train_data.dropna(subset=['sql_prompt', 'sql_context', 'sql'])\n",
    "\n",
    "#Tạo input_text kết hợp câu hỏi và ngữ cảnh\n",
    "total_train_data['input_text'] = total_train_data.apply(\n",
    "    lambda row: f\"Translate SQL: {row['sql_prompt']} Context: {row['sql_context']}\", axis=1\n",
    ")\n",
    "\n",
    "#Cắt thành bộ dữ liệu thử \n",
    "train_data = total_train_data.head(1000)\n",
    "test_data = total_train_data[1000:1100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mã hóa dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "\n",
    "train_texts = train_data['input_text'].tolist()\n",
    "train_labels = train_data['sql'].tolist()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "train_labels_enc = tokenizer(train_labels, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Thêm nhãn vào dữ liệu mã hóa\n",
    "train_encodings['labels'] = train_labels_enc['input_ids']\n",
    "\n",
    "# Chuyển đổi thành tập Dataset\n",
    "train_dataset = Dataset.from_dict(train_encodings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Khởi tạo mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tạo input_text cho dữ liệu test\n",
    "test_data['input_text'] = test_data.apply(\n",
    "    lambda row: f\"Translate SQL: {row['sql_prompt']} Context: {row['sql_context']}\", axis=1\n",
    ")\n",
    "\n",
    "# 2. Mã hóa dữ liệu test\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tải mô hình đã huấn luyện\n",
    "model = T5ForConditionalGeneration.from_pretrained('./results')\n",
    "\n",
    "# 3. Dự đoán với mô hình cho tập test\n",
    "predictions = []\n",
    "total_correct = 0\n",
    "total_samples = len(test_data)\n",
    "for idx, text in enumerate(test_data['input_text']):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model.generate(**inputs)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if(prediction==test_data['sql']):\n",
    "        total_correct += 1\n",
    "        print('Test ' + idx + ' correct')\n",
    "\n",
    "accuracy = total_correct/total_samples       \n",
    "\n",
    "print('Accuracy: ' + accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
